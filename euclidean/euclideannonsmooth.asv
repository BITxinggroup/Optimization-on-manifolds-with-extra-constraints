function [gradnorms, alphas, xCur, time] = euclideannonsmooth(problem, x, options)
    
    timetic = tic();
    M = problem.M;
    dim = M.dim();

    if ~exist('x','var')|| isempty(x)
        xCur = x;
    else
        xCur = M.rand();
    end

    xCurGradient = getGradient(problem, xCur);
    xCurGradNorm = M.norm(xCur, xCurGradient);
    xCurCost = getCost(problem, xCur);
    
    gradnorms = zeros(1,1000);
    gradnorms(1,1) = xCurGradNorm;
    alphas = zeros(1,1000);
    alphas(1,1) = 1;
    stepsizes = zeros(1,1000);
    stepsizes(1,1) = NaN;

    options.error = 1e-7;
    options.linesearchVersion = 4;
%     options.memory = 10;

    Hcore = eye(dim);
    Hside  = zeros(dim,dim);
    H = Hcore;
    
    k = 0;
    iter = 0;
    alpha = 1;
    scaleFactor = 1;
    stepsize = 1;

    fprintf(' iter\t               cost val\t    grad. norm\t   alpha \n');

    while (1)
        %_______Print Information and stop information________
        fprintf('%5d\t%+.16e\t%.8e\t%f\n', iter, xCurCost, xCurGradNorm, alpha);

        if (xCurGradNorm < options.error)
            fprintf('Target Reached\n');
            break;
        end
        if (stepsize <= 1e-10)
            fprintf('Stepsize too small\n')
            break;
        end

        %_______Get Direction___________________________

        p = -H*xCurGradient;

        %_______Line Search____________________________

        if options.linesearchVersion == 0
            alpha = 0.5;
            [alpha, xNext, xNextCost] = linesearchBFGS(problem,...
                xCur, p, xCurCost, M.inner(xCur,xCurGradient,p), alpha); %Check if df0 is right
            step = M.lincomb(xCur, alpha, p);
            stepsize = M.norm(xCur, p)*alpha;
        elseif options.linesearchVersion == 1
            [stepsize, xNext, newkey, lsstats] =linesearch(problem, xCur, p, xCurCost, M.inner(xCur,xCurGradient,p));
            alpha = stepsize/M.norm(xCur, p);
            step = M.lincomb(xCur, alpha, p);
            xNextCost = getCost(problem, xNext);
        elseif options.linesearchVersion == 2
            [xNextCost,alpha] = linesearchv2(problem, M, xCur, p, M.inner(xCur,xCurGradient,p), alpha);
            step = M.lincomb(xCur, alpha, p);
            stepsize = M.norm(xCur, step);
            xNext = M.retr(xCur, step, 1);
        elseif options.linesearchVersion == 3
            alpha = 1;
            step = M.lincomb(xCur, alpha, p);
            stepsize = M.norm(xCur, step);
            xNext = M.retr(xCur, step, 1);
            xNextCost = getCost(problem, xNext);
        else
            [xNextCost, alpha] = linesearchnonsmooth(problem, M, xCur, p, xCurCost, M.inner(xCur,xCurGradient,p), alpha);
            step = M.lincomb(xCur, alpha, p);
            stepsize = M.norm(xCur, step);
            xNext = M.retr(xCur, step, 1);            
        end
        
        %_______Updating the next iteration_______________
        xNextGradient = getGradient(problem, xNext);
        sk = step; 
        yk = xNextGradient - xCurGradient; 

        scaleFactor = (sk.'*yk) / (yk.'*yk);
        
        Hcore =
        Hside = 
        H = Hcore*scalreFactor + Hside;
        
        
        
        
        
        iter = iter + 1;
        xCur = xNext;
        xCurGradient = xNextGradient;
        xCurGradNorm = M.norm(xCur, xNextGradient);
        xCurCost = xNextCost;
        
        gradnorms(1,iter+1)= xCurGradNorm;
        alphas(1,iter+1) = alpha;
    end
    
    gradnorms = gradnorms(1,1:iter+1);
    alphas = alphas(1,1:iter+1);
    time = toc(timetic);
end

function dir = getDirection(M, xCur, xCurGradient, sHistory, yHistory, rhoHistory, scaleFactor, k)
    q = xCurGradient;
    inner_s_q = cell(1, k);
    for i = k : -1: 1
        inner_s_q{i} = rhoHistory{i}*M.inner(xCur, sHistory{i},q);
        q = M.lincomb(xCur, 1, q, -inner_s_q{i}, yHistory{i});
    end
    r = M.lincomb(xCur, scaleFactor, q);
    for i = 1: k
         omega = rhoHistory{i}*M.inner(xCur, yHistory{i},r);
         r = M.lincomb(xCur, 1, r, inner_s_q{i}-omega, sHistory{i});
    end
    dir = M.lincomb(xCur, -1, r);
end


function [alpha, xNext, xNextCost] = ...
                  linesearchBFGS(problem, x, d, f0, df0, alphaprev)

    % Backtracking default parameters. These can be overwritten in the
    % options structure which is passed to the solver.
    contraction_factor = .5;
    optimism = 1/.5;
    suff_decr = 1e-4;
%     suff_decr = 0;
    max_steps = 25;
    
    % At first, we have no idea of what the step size should be.
    alpha = alphaprev * optimism;

    % Make the chosen step and compute the cost there.
    xNext = problem.M.retr(x, d, alpha);
    xNextCost = getCost(problem, xNext);
    cost_evaluations = 1;
    
    % Backtrack while the Armijo criterion is not satisfied
    while xNextCost > f0 + suff_decr*alpha*df0
        
        % Reduce the step size,
        alpha = contraction_factor * alpha;
        
        % and look closer down the line
        xNext = problem.M.retr(x, d, alpha);
        xNextCost = getCost(problem, xNext);
        cost_evaluations = cost_evaluations + 1;
        
        % Make sure we don't run out of budget
        if cost_evaluations >= max_steps
            break;
        end
        
    end
    
    % If we got here without obtaining a decrease, we reject the step.
    if xNextCost > f0
        alpha = 0;
        xNext = x;
        xNextCost = f0; 
    end
    
%     fprintf('alpha = %.16e\n', alpha)
end


function [costNext,alpha] = linesearchv2(problem, M, x, d, df0, alphaprev)

    alpha = alphaprev;
    costAtx = getCost(problem,x);
    while (getCost(problem,M.exp(x,d,2*alpha))-costAtx < alpha*df0)
        alpha = 2*alpha;
    end
    costNext = getCost(problem,M.exp(x,d,alpha));
    diff = costNext - costAtx;
    while (diff>= 0.5*alpha*df0)
        if (diff == 0)
            alpha = 0;
            break;
        end
        alpha = 0.5 * alpha;
        costNext = getCost(problem,M.exp(x,d,alpha));
        diff = costNext - costAtx;
    end
%     fprintf('alpha = %.16e\n',alpha);    
end

function [costNext, t] = linesearchnonsmooth(problem, M, xCur, d, f0, df0, alphaprev)
    alpha = 0;
    beta = inf;
    t = 1;
    c1 = 0.1; %need adjust
    c2 = 0.9; %need adjust.
    counter = 100;
    while counter > 0
        xNext = M.retr(xCur, d, t);
        if (getCost(problem, xNext) > f0 + df0*c1*t)
            beta = t;
        elseif diffRetractionEuclidean(problem, M, alpha, d, xCur, xNext) < c2*df0
            alpha = t;
        else
            break;
        end
        if (isinf(beta))
            t = alpha*2;
        else
            t = (alpha+beta)/2;
        end
        counter = counter - 1;
    end
    costNext = getCost(problem, xNext);
end

function val = diffRetractionEuclidean(problem, M, alpha, d, xCur, xNext)
    val = M.inner(xNext, getGradient(problem, xNext), d);
end